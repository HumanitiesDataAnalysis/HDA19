---
title: "Texts, Functions, and Probabilities"
author: "Ben Schmidt"
date: "2019-02-14"
output: pdf_document
---


```{r global_options, include=FALSE}
library(knitr)
library(magrittr)
library(ggplot2)
library(dplyr)

#opts_chunk$set(eval=FALSE, warning=FALSE, message=FALSE)
```

# Texts

Research using texts is perhaps the most defining feature of digital humanities work. We're going to start by looking at one of the most canonical sets of texts out there: the State of the Union Addresses.

We're also going to dig a little deeper into two important aspects of R we've ignored so far: **functions** (in the sense that you can write your own) and **probability**.

# Reading Text into R

You can read in a text the same way you would a table. But it's easier to simply read it in as a character vector. (Note: you can find a process very similar to this at greater length in the Jockers text analysis book online.)

``` {r}
library(tidyverse)
text = read_lines("../data/SOTUS/2019.txt")
```

If you type in "text," you'll see that we get the full 2015 State of the Union address.

As structured here, the text is divided into *paragraphs.* For most of this class, we're going to be interested instead in working with *words*.

### Tokenization.

Let's pause and note a perplexing fact: there's no good definition of a word. In the field of corpus linguistics, the term is ignored in favor of the idea of a "token" or "type." Where a word is more abstract, a "token" is a concrete term used in actual language, and a "type" is the particular string we're interested in.

Breaking a piece of text into words is thus called "tokenization." There are many ways to do it--coming up with creative tokenization methods will be helpful in the algorithms portion of this class. But the simplest is to simply to remove anything that isn't a letter. Using regular expression syntax, the R function `strsplit` lets us do just this: split a string into pieces. We could use the regular expression `[^A-Za-z]` to say "split on anything that isn't a letter between A and Z." Note, for example, that this makes the word "Don't" into two words.

``` {r}
text %>% 
  str_split("[^A-Za-z]") %>%
  head(5)

```

You'll notice that now each paragraph is broken off in a strange way. We're now working with data structures other than a data.frame. This can be useful: but doesn't let us apply the rules of tidy analysis. We've been working with, instead, `tibble` objects in `dplyr.` This individual word, though, can be turned into a column in a tibble by using the function to create it. 


``` {r}
SOTU = tibble(text = text)
```

Now we can use the "tidytext" package to start to analyze the document.

The workhorse function in `tidytext` is `unnest_tokens`. It creates a new columns (here called 'words')
from each of the individual ones in text. This is the same, conceptually, as the 

```{r}

library(tidytext)

tidied = SOTU %>% 
  unnest_tokens(word, text)
  
```

What can we do with such a column put into a data.frame?

First off, you should be able to see that the good old combination of `group_by`, `summarize`, and `n()` allow us to create a count of words in the document.

This is perhaps the time to tell you that there is a shortcut in `dplyr` to do all of those at once: the `count` function.

``` {r}
wordcounts = tidied %>% group_by(word) %>% summarize(count=n()) %>% arrange(-count)
wordcounts

```

Using ggplot, we can plot the most frequent words.

``` {r}
wordcounts = wordcounts %>% mutate(rank = rank(-count))  %>% filter(count>2,word!="")

ggplot(wordcounts) + aes(x=rank,y=count,label=word) + geom_text()
```

As always, you should experiment with multiple scales. Putting logarithmic scales on both axes reveals something interesting:

``` {r}


ggplot(wordcounts) + aes(x=rank,y=count,label=word) + geom_text() + scale_x_continuous(trans="log") + scale_y_continuous(trans="log")

```

The logarithm of rank decreases linearily with the logarithm of count. 

This is "Zipf's law:" the phenomenon means that the most common word is twice as common as the second most common word, three times as common as the third most common word, four times as common as the fourth most common word, and so forth. 

It is named after the linguist George Zipf, who first found the phenomenon while laboriously counting occurrences of individual words in Joyce's *Ulysses* in 1935.

This is a core textual phenomenon, and one you must constantly keep in mind: common words are very common indeed, and logarithmic scales are more often appropriate for plotting than linear ones. This pattern results from many dynamic systems where the "rich get richer," which characterizes all sorts of systems in the humanities. Consider, for one last time, our city population data.

``` {r}

cities = read_csv("../data/CESTACityData.csv")
head(cities)
nowadays = cities %>% select(CityST,`2010`) %>% arrange(-`2010`) %>% mutate(rank = rank(-`2010`)) %>% filter(rank<1000)

ggplot(nowadays) + aes(x=rank,label=CityST,y=`2010`) + geom_text() + scale_x_log10() + scale_y_log10()

```

It shows the same pattern!

## Concordances

This data frame can also build what used to be the effort of entire scholarly careers: A "concordance." We do this by adding a second column to the frame which is not just the first word, but the second. `dplyr` includes a `lag` and `lead` function that let you combine the next element. You specify by how many positions you want a vector to "lag" or "lead" another one.

``` {r}
numbers = c(1,2,3,4,5)
lag(numbers,1)
lead(numbers,1)
```

By using `lag` on a character vector, we can neatly align one series of text with the words that follow. By grouping on both words, we can use that to count bigrams:

``` {r}
twoColumns = tidied %>% mutate(word2 = lead(word,1))
twoColumns %>% group_by(word, word2) %>%
  summarize(count=n()) %>%
  arrange(-count)
```


Doing this several times gives us snippets of the text we can read *across* as well as down.

``` {r}
multiColumn = tidied %>% mutate(word2 = lead(word,1),word3=lead(word,2),word4=lead(word,3))
multiColumn %>% head
```

Using `filter`, we can see the context for just one particular word.

``` {r}
multiColumn %>% filter(word3=="congress")
```

# Functions

That's just one State of the Union. How are we to read them all in?

We could obviously type all the code in, one at a time. But that's bad pogramming!

To work with this, we're going to finally introduce a core programming concepts: **functions.**

We've used functions, continuously: but whenever you've written up a useful batch of code, you can bundle it into a function that you can then reuse.

Here, for example, is a function that will read the state of the union address for any year.

Note that we add one more thing to the end--a column that identifies the year.

```{r}
readSOTU = function(filename) {
  
    tibble(text = .) %>% 
    filter(text != "") %>%
    # Add a counter for the paragraph number
    mutate(paragraph = 1:n()) %>%
    unnest_tokens(word, text) %>%
    mutate(filename = filename %>% str_replace(".*/", "") %>% 
             str_replace(".txt", ""))
}
```



We also need to know all the names of the files!

```{r}
all_files = list.files("../data/SOTUS/", full.names = T)
```

Now we have a list of State of the Unions and a function to read them in. How do we automate this procedure? There are several ways.

The most traditional way, dating back to the programming languages of the 1950s, would be to write a `for` loop. This is OK to do, and maybe you'll see it at some point. But it's not the approach we take in this class.

```{r}
allSOTUs=tibble()
for (fname in all_files) {
  cat(fname)
  allSOTUs = rbind(allSOTUs,readSOTU(fname))
}
```

This works, but is relatively slow. Good R programmers almost *never* write a `for-loop`; instead, they use functions that allow you operate on lists. One of the most basic ones is called "map": it takes as an argument a list and a function, and applies the function to each element of the list. We can then collapse the list down to a `tibble` using the `bind_rows` function.

We can get a list of years in R using a colon.

``` {r}
all = map(all_files, readSOTU) %>% bind_rows

all %>% group_by(filename) %>% summarize(count=n()) %>% 
  ggplot() + geom_line() + aes(x=filename %>% as.numeric, y = count)

```

Either of these are acceptable: but if you want to remain entirely in the `dplyr` idiom, the way to do this is *create a data frame* and then apply a function to it.

`dplyr` uses the function `do` to describe this. The only tricky thing about `do` is that it requires you to use `dplyr`'s special character, the period, to describe the grouped frame. So to run our `readSOTU` function, we say `do(readSOTU.$file)`.

One nice side effect here is that you get a progress bar.


``` {r}
allSOTUs = tibble(file = all_files) %>%
  group_by(file) %>% 
  do(readSOTU(.$file)) %>%
  mutate(year = as.numeric(filename))


```

## Metadata Joins

The metadata here is edited from the [Programming Historian](http://programminghistorian.github.io/ph-submissions/lessons/published/basic-text-processing-in-r)

```{r}
metadata = read_csv("../data/SOTUmetadata.csv")
allSOTUs = metadata %>% inner_join(allSOTUs) 
allSOTUs %>%
  ggplot() + 
  geom_bar(aes(x=year, fill=party))
```



```{r}

allSOTUs %>%
  ggplot() + 
  geom_point(aes(x=year, fill=sotu_type))

```

One nice application is finding words that are unique to each individual author. We can do that to find words that only appeared in a single State of the Union. We'll put in a quick lowercase function so as not to worry about capitalization.

``` {r}

allSOTUs %>% 
  mutate(year = as.numeric(filename)) %>%
  group_by(word, president) %>% 
  summarize(count=n()) %>%
  group_by(word) %>% 
  filter(n()==1) %>% 
  arrange(-count)

```

## Advanced comparison

But there are many other methods. For specific variations of what it means to ask "what books use word x more", you can think about TF-IDF, Dunning Log-likelihood, or Mann-Whitney scores.  


```{r}
president_counts = allSOTUs %>%
  group_by(president, word) %>%
  summarize(count = n())
```

# Probabilities

Let's look at probabilities, which lets us apply our merging and functional skills in a fun way.

``` {r}

bigrams = allSOTUs %>% mutate(word2=lead(word,1),word3=lead(word,2))

transitions = bigrams %>% 
  group_by(word) %>% 
  # First, we store the count for each word.
  mutate(word1Count=n()) %>% 
  group_by(word,word2) %>%
  # Then we group by the second word to see what share of the first word is followed
  # by the second.
  summarize(chance = n()/word1Count[1])
```

This gives a set of probabilities. What words follow "United?"

```{r}
transitions %>% filter(word=="united") %>% arrange(-chance)
```

We can use the `weight` argument to `sample_n`. If you run this several times, you'll see that you get different results.

```{r}
transitions %>% filter(word=="my") %>% sample_n(1, weight = chance)
```

So now, consider how we combine this with joins. If we make a new column.

```{r}
seed = c("my")

frame = data_frame(word = seed)

frame %>%
  inner_join(transitions) %>%
  sample_n(1, weight = chance)
```

If we wanted to *extend* the original frame, we can rename the rows and
use the `rbind` function to attach it to the end.

```{r}
frame %>%
  inner_join(transitions) %>%
  sample_n(1, weight = chance) %>% 
  # We rename: what used to be word2 can go in the word column
  select(word = word2)  %>%
  bind_rows(frame, .)
```

Why is that useful? Because now we can combine this join and the wordcounts to keep doing the same process!

```{r}
add_a_row = function(frame, transitions) {
  next_row = frame %>% 
    # we're extending just the last line.
    tail(1) %>% 
    inner_join(transitions, by=intersect(names(.), names(transitions))) %>%
    sample_n(1, weight = chance) %>%
    # Rather than select just the second word, we drop the
    # other two. This makes the function extensible to longer chains.
    select(-word, -chance)
  
  # We have to give the original names back.
  names(next_row) = names(frame)
  
  # Return them bound back together
  bind_rows(frame, next_row)
}

tibble(word = "america") %>%
  add_a_row(transitions) %>%
  add_a_row(transitions)  %>%
  add_a_row(transitions) %>%
  add_a_row(transitions) %>%
  add_a_row(transitions)

```

```{r}

length_2_transitions = allSOTUs %>% 
  mutate(word2 = lag(word, 1), word3 = lag(word, 2)) %>%
  group_by(word, word2, word3) %>% 
  summarize(chance=n())

length_2_transitions %>% filter(word == "my", word2 == "fellow")

tibble(word = "my", word2 = "fellow") %>%
  add_a_row(length_2_transitions)

```


``` {r}
word = "i"
output = data_frame(word = "i")

while(nrow(output) < 30) {
  output = add_a_row(output, transitions)  
  tail = output %>% tail(1) %>% pull(word)
  cat(tail, " ")
}

```



How would we make the same function work over longer stretches? 

First, we'd just make the transition probabilities apply over stretches of three words. That's the same general principle, but we divide the count of word3 by the bigram count that precedes it.

``` {r}
transitions = allSOTUs %>% 
  filter(year>1912) %>% 
  group_by(word) %>% 
  mutate(word1Count=n()) %>% 
  group_by(word,word2) %>% 
  mutate(chance = n()/word1Count,bigramCount=n()) %>% 
  group_by(word,word2,word3) %>% 
  summarize(trigramChance = n()/bigramCount[1])

```

Then, we'd make the "find next word" function work over spans of two words instead of one.

``` {r}

findNextWord = function(previous,current) {
  subset = transitions %>% 
    filter(word==previous,word2==current)
  nextWord = sample(subset$word3,1,prob = subset$trigramChance)
  return(nextWord)
}
```

Finally, we have to remember both the "current" and "previous" word at all times in the loop: and we have to start it with two words instead of one. But with this, we get some wonderfully serendipitous nonsense.


``` {r}

previous = "We"
current = "must"
while (TRUE) {
  message(previous)
  nextWord = findNextWord(previous,current)
  previous = current
  current = nextWord
}

```

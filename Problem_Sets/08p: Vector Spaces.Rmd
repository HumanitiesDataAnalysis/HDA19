---
title: "8p-Vector Spaces"
author: "Ben Schmidt"
date: "2019-02-28"
output: pdf_document
---

# 1. Basic re-productions on your own texts.

1. Adapt from last week's handout: use it to read in a set of documents of your own. How big is your matrix?


```{r}


```


2. Use a filter so that you get about 200 columns in the matrix, normalized by the number of words.

``` {r}

```

4. Run your own Principal Components Analysis and plot them along the lines of the example or the Allison-Heuser pamphlet we read: as either a point chart using `shape` and `color` as aesthetics on geom_point or as a geom_label.

5. Look at the loadings for those results by examining your model's weights and try to explain why PCA segregates in the way that it does.

``` {r}

```

6. (Optional) Before running PCA, try reducing the vocabulary down significantly. Drop out stopwords, for instance, or only look at capitalized words to capture names. 

``` {r}

```


7. Choose a document you're particularly interested in: use the `dist` function to calculate what the documents it shares the closest profile with. Does this make sense? Are there signs of overfitting?

# 2. Expanding the domain

8. Principal components need not only be run on wordcounts: it can be done on anything with several different numeric dimensions. Try running it with a dataset whose columns are *not* word counts. **The easiest choice would be to go back and look at the populations in each census, treating rows as cities and columns as years.** A harder version would be population percent change, or something involving the library set. E-mail if you want some ideas. If you take the road less traveled, post it on the blog so we can talk about it in class next week.

**If you choose cities, you will want to normalize.** Instead of using the population in each year, use the *total percentage of the city's population that was there in year X. This shouldn't be too hard to do with `group_by` and `mutate`: if you're having trouble, e-mail the class.


``` {r}

```


# Word2Vec

Train a word2vec model on your corpus, and look at similar words using functions derived from the Week 8 handout.
